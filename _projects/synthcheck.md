---
layout: page
title: SynthCheck
description: AI model for Synthetic vs. Real Image classification
img: assets/img/dataset-cover.png
importance: 1
category: work
related_publications: true
---

In an era where digital content can be crafted and manipulated with alarming ease, the line between reality and artificiality blurs, presenting not only technological advancements but also ethical dilemmas and potential dangers. Deepfake technology, a profound testament to AI's capabilities, allows for the creation of incredibly realistic images and videos that are nearly indistinguishable from authentic content. This rise in synthetic media poses significant risks, from misinformation to identity theft, making it imperative to develop robust mechanisms for discerning these AI-generated fabrications from genuine images.

<h2><b>The Challenge of Synthetic Image Detection</b></h2>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/AI-Generated.jpg" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

Imagine looking at this image of a group of friends walking down the street under evening lights. At first glance, the image looks genuine, capturing details and textures as a photo should. Yet, it is synthetically generated by AI. The subtle quality of AI-generated images makes it nearly impossible for even a trained eye to distinguish between the two, highlighting the complexity of the task.

<h2><b>Model Architecture and Setup</b></h2>

<p>For this project, ResNet-50 was selected for its high accuracy and adaptability in classification tasks, making it well-suited for distinguishing real images from synthetically generated ones. The following key steps and modifications were made to optimize the model for this binary classification challenge:</p>

<h4><b>Pre-trained Backbone</b></h4>
<p>Leveraging ResNet-50’s pre-trained weights on the ImageNet dataset provided a robust foundation, as the model already had extensive knowledge of general object features and textures. This initial setup minimized the need for extensive retraining, enabling the network to focus specifically on identifying subtle visual artifacts common in AI-generated images.</p>
<h4><b>Fine-tuning</b></h4>
<p>The ResNet-50 model was fine-tuned for the binary classification task by modifying its last layer to a single output node, designed to output the likelihood of an image being real or synthetic. The model’s ResNet-50 backbone was initialized with pre-trained weights (pretrained=True), leveraging existing knowledge from ImageNet and reducing the need for training from scratch. The model was trained on a CUDA-enabled device (when available), allowing for efficient parallel computation.</p>
<h4><b>Data Splitting and Augmentation</b></h4>
<p>The Artifact-CycleGAN dataset, consisting of 15,000 images evenly split into real and synthetic categories, was used. An 80-20 train-validation split provided a reliable evaluation set while maintaining a diverse training base.</p>


<h2><b>Training and Optimization Process</b></h2>
<p>Initial training runs for the SynthCheck project highlighted the need for further optimization to stabilize the learning curve and improve accuracy.</p>

<h4><b>Learning Rate and Optimizer Selection</b></h4>
<p>Using the Adam optimizer with an initial learning rate of 0.0001 allowed for stable convergence across epochs. This learning rate was selected to balance training speed and model stability, preventing oscillations in the loss and ensuring a steady improvement in accuracy.</p>

<h4><b>Loss Function</b></h4>
<p>Binary Cross-Entropy with Logits Loss (BCEWithLogitsLoss) was employed to handle the binary classification task, where the model needed to predict a single probability for each image, indicating whether it was real or AI-generated. This configuration minimized misclassifications and improved confidence in binary predictions.</p>

Below are the key graphical representations of our project's outcomes:

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/trainval-accuracyloss.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>

<div class="caption">
The left graph displays the training and validation loss per epoch, indicating how the model's error rates decreased over time. The right graph shows the training and validation accuracy per epoch, illustrating improvements in the model's ability to correctly classify images.
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/confusion_matrix.png" title="example image" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
The plot illustrates the performance of the classifier in distinguishing real images from AI-generated ones. The diagonal cells represent correctly classified images (True Positives and True Negatives), while the off-diagonal cells indicate misclassifications (False Positives and False Negatives).
</div>

<b>Kaggle notebook:</b> <a href="https://www.kaggle.com/code/mithildamani/synthcheck/">here</a>.